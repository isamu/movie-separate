import dotenv from 'dotenv';
import path from 'path';
import { promises as fs } from 'fs';
import {
  ensureOutputDir,
  extractAudioFromVideo,
  splitVideo,
  splitAudio,
  getVideoDuration,
  generateThumbnail,
} from './ffmpeg-utils.js';
import { segmentVideo } from './segmentation.js';
import { transcribeAudioBilingual, identifySpeakers, textToSpeech } from './transcription.js';
import { evaluateSegments } from './evaluation.js';
import { Beat, Output, MultiLinguals } from './types.js';
import { parseArgs } from './cli.js';
import { getConcurrencyConfig, createApiLimiters } from './concurrency.js';

dotenv.config();

const CONCURRENCY = getConcurrencyConfig();
const API_LIMITERS = createApiLimiters(CONCURRENCY);

const OUTPUT_DIR = 'output';

// „Ç≥„Éû„É≥„Éâ„É©„Ç§„É≥ÂºïÊï∞„Çí„Éë„Éº„Çπ
const cliOptions = parseArgs(process.argv);
const INPUT_VIDEO = cliOptions.input;
const DEFAULT_LANG = cliOptions.lang;
const TEST_MODE = cliOptions.test;
const TEST_DURATION = cliOptions.testDuration;

function populateCacheMaps(
  beats: Beat[],
  translationMap: Map<string, string>,
  beatMap: Map<string, Beat>
) {
  beats.forEach((beat) => {
    if (beat.multiLinguals?.ja && beat.multiLinguals?.en) {
      translationMap.set(beat.multiLinguals.ja, beat.multiLinguals.en);
    }
    if (beat.videoSource) {
      beatMap.set(beat.videoSource, beat);
    }
  });
}

async function loadExistingCache(outputPath: string) {
  const existingTranslations = new Map<string, string>();
  const existingBeatsCache = new Map<string, Beat>();

  try {
    const existingData = await fs.readFile(outputPath, 'utf-8');
    const existingOutput: Output = JSON.parse(existingData);
    populateCacheMaps(existingOutput.beats, existingTranslations, existingBeatsCache);

    if (existingBeatsCache.size > 0) {
      console.log(`‚ôªÔ∏è  Loaded ${existingBeatsCache.size} existing segments from cache`);
      console.log(`   - ${existingTranslations.size} translations`);
    }
  } catch {
    console.log('üìù No existing cache found, starting fresh');
  }

  return { existingTranslations, existingBeatsCache };
}

async function generateVideoAndThumbnail(
  videoOutput: string,
  thumbnailOutput: string,
  inputVideo: string,
  start: number,
  duration: number
) {
  try {
    await fs.access(videoOutput);
    await fs.access(thumbnailOutput);
    console.log(`  ‚ôªÔ∏è  Video and thumbnail already exist, skipping generation`);
    return false;
  } catch {
    console.log(`  üìπ Splitting video...`);
    await splitVideo(inputVideo, videoOutput, start, duration);
    console.log(`  üñºÔ∏è  Generating thumbnail...`);
    await generateThumbnail(videoOutput, thumbnailOutput, 0);
    return true;
  }
}

async function extractAudioIfNeeded(
  audioOutput: string,
  inputVideo: string,
  start: number,
  duration: number
) {
  try {
    await fs.access(audioOutput);
    console.log(`  ‚ôªÔ∏è  Audio file already exists, skipping extraction`);
    return false;
  } catch {
    console.log(`  üéµ Extracting audio...`);
    await splitAudio(inputVideo, audioOutput, start, duration);
    return true;
  }
}

async function getTranscriptionAndTranslation(
  cachedBeat: Beat | undefined,
  audioOutput: string,
  existingTranslations: Map<string, string>,
  sourceLang: string
): Promise<MultiLinguals> {
  if (cachedBeat?.multiLinguals?.ja && cachedBeat?.multiLinguals?.en) {
    console.log(`  ‚ôªÔ∏è  Transcription and translation cached, skipping Whisper & Translation API`);
    return cachedBeat.multiLinguals;
  }

  console.log(`  üìù Transcribing audio (${sourceLang})...`);
  const multiLinguals = await transcribeAudioBilingual(audioOutput, sourceLang, existingTranslations);
  const targetLang = sourceLang === 'en' ? 'ja' : 'en';
  console.log(`  ‚úÖ Transcription (${sourceLang.toUpperCase()}): ${multiLinguals[sourceLang as 'en' | 'ja'].substring(0, 80)}...`);
  console.log(`  ‚úÖ Translation (${targetLang.toUpperCase()}): ${multiLinguals[targetLang as 'en' | 'ja'].substring(0, 80)}...`);
  return multiLinguals;
}

async function identifySpeaker(
  cachedBeat: Beat | undefined,
  multiLinguals: MultiLinguals
): Promise<string> {
  if (cachedBeat?.speaker) {
    console.log(`  ‚ôªÔ∏è  Speaker identification cached, skipping GPT-4o API`);
    return cachedBeat.speaker;
  }

  console.log(`  üë• Identifying speakers...`);
  const speakerSegments = await identifySpeakers(multiLinguals.ja);
  return speakerSegments.length > 0 ? speakerSegments[0].speaker : 'Unknown Speaker';
}

async function generateJapaneseTTS(jaAudioOutput: string, text: string) {
  try {
    await fs.access(jaAudioOutput);
    console.log(`  ‚ôªÔ∏è  Japanese TTS audio already exists, skipping TTS API`);
    return false;
  } catch {
    console.log(`  üé§ Generating Japanese TTS audio...`);
    await textToSpeech(text, jaAudioOutput, 'ja');
    return true;
  }
}

function displayEvaluationStats(beats: Beat[]) {
  const highImportance = beats.filter(b => (b.importance || 0) >= 7).length;
  const mediumImportance = beats.filter(b => (b.importance || 0) >= 4 && (b.importance || 0) < 7).length;
  const lowImportance = beats.filter(b => (b.importance || 0) < 4).length;

  console.log(`\nüìà Importance Distribution:`);
  console.log(`   High (7-10): ${highImportance} segments`);
  console.log(`   Medium (4-6): ${mediumImportance} segments`);
  console.log(`   Low (0-3): ${lowImportance} segments`);
}

function createBeatFromSegment(
  segmentNum: number,
  segment: { start: number; end: number },
  multiLinguals: MultiLinguals,
  mainSpeaker: string
): Beat {
  const duration = segment.end - segment.start;
  return {
    text: multiLinguals.en,
    audioSources: { en: `${segmentNum}.mp3`, ja: `${segmentNum}_ja.mp3` },
    multiLinguals: multiLinguals,
    videoSource: `${segmentNum}.mp4`,
    thumbnail: `${segmentNum}.jpg`,
    speaker: mainSpeaker,
    startTime: segment.start,
    endTime: segment.end,
    duration: duration,
  };
}

interface SegmentProcessingContext {
  segment: { start: number; end: number };
  segmentNum: number;
  totalSegments: number;
  videoOutputDir: string;
  existingBeatsCache: Map<string, Beat>;
  existingTranslations: Map<string, string>;
  sourceLang: string;
}

async function processSegmentPhase1(ctx: SegmentProcessingContext): Promise<Beat> {
  const { segment, segmentNum, totalSegments, videoOutputDir, existingBeatsCache, existingTranslations, sourceLang } = ctx;
  const duration = segment.end - segment.start;
  console.log(`\nüéûÔ∏è  Processing segment ${segmentNum}/${totalSegments} (${segment.start.toFixed(1)}s - ${segment.end.toFixed(1)}s, duration: ${duration.toFixed(1)}s)...`);

  const videoOutput = path.join(videoOutputDir, `${segmentNum}.mp4`);
  const audioOutput = path.join(videoOutputDir, `${segmentNum}.mp3`);
  const thumbnailOutput = path.join(videoOutputDir, `${segmentNum}.jpg`);
  const cachedBeat = existingBeatsCache.get(`${segmentNum}.mp4`);

  await generateVideoAndThumbnail(videoOutput, thumbnailOutput, INPUT_VIDEO, segment.start, duration);
  await extractAudioIfNeeded(audioOutput, INPUT_VIDEO, segment.start, duration);
  const multiLinguals = await getTranscriptionAndTranslation(cachedBeat, audioOutput, existingTranslations, sourceLang);
  const mainSpeaker = await identifySpeaker(cachedBeat, multiLinguals);
  return createBeatFromSegment(segmentNum, segment, multiLinguals, mainSpeaker);
}

async function saveProgress(
  outputPath: string,
  beats: Beat[],
  processDuration: number,
  totalSegments: number
) {
  const output: Output = {
    lang: DEFAULT_LANG,
    totalDuration: processDuration,
    totalSegments: totalSegments,
    beats: beats,
  };
  await fs.writeFile(outputPath, JSON.stringify(output, null, 2), 'utf-8');
  console.log(`  üíæ Saved progress to ${path.basename(outputPath)}`);
}

async function main() {
  console.log('üé¨ Starting video processing...');
  console.log(`üìπ Input video: ${INPUT_VIDEO}`);
  console.log(`üåê Default language: ${DEFAULT_LANG}`);

  if (TEST_MODE) {
    console.log('üß™ TEST MODE: Processing first 5 minutes only');
  }

  // ÂÖ•ÂäõÂãïÁîª„Éï„Ç°„Ç§„É´Âêç„Åã„ÇâÊã°ÂºµÂ≠ê„ÇíÈô§„ÅÑ„Åü„Éô„Éº„ÇπÂêç„ÇíÂèñÂæó
  const videoBaseName = path.basename(INPUT_VIDEO, path.extname(INPUT_VIDEO));

  // Âá∫Âäõ„Éá„Ç£„É¨„ÇØ„Éà„É™„ÇíÂãïÁîªÂêç„Å´Âü∫„Å•„ÅÑ„Å¶‰ΩúÊàê (‰æã: output/ai/)
  const videoOutputDir = path.join(OUTPUT_DIR, videoBaseName);
  await ensureOutputDir(videoOutputDir);
  console.log(`üìÅ Output directory: ${videoOutputDir}`);

  // Êó¢Â≠ò„ÅÆ„Éá„Éº„Çø„Çí„É≠„Éº„ÉâÔºàÂ≠òÂú®„Åô„ÇãÂ†¥ÂêàÔºâ
  const outputPath = path.join(videoOutputDir, 'mulmo_view.json');
  const { existingTranslations, existingBeatsCache } = await loadExistingCache(outputPath);

  // ÂãïÁîª„ÅÆÂÖ®‰Ωì„ÅÆÈï∑„Åï„ÇíÂèñÂæó
  const totalDuration = await getVideoDuration(INPUT_VIDEO);
  const processDuration = TEST_MODE ? Math.min(totalDuration, TEST_DURATION) : totalDuration;

  console.log(
    `üìä Total video duration: ${totalDuration.toFixed(2)}s (${(totalDuration / 60).toFixed(2)} minutes)`
  );

  if (TEST_MODE) {
    console.log(
      `üìä Processing duration: ${processDuration.toFixed(2)}s (${(processDuration / 60).toFixed(2)} minutes)`
    );
  }

  // ÂãïÁîª„ÅÆÈï∑„Åï„ÇíÂèñÂæó„Åó„Å¶„Çª„Ç∞„É°„É≥„Éà„Å´ÂàÜÂâ≤
  console.log('üìä Analyzing video and creating segments...');
  const allSegments = await segmentVideo(INPUT_VIDEO, 20, 120);

  // „ÉÜ„Çπ„Éà„É¢„Éº„Éâ„ÅÆÂ†¥Âêà„ÅØÊúÄÂàù„ÅÆ5ÂàÜ„ÅÆ„Çª„Ç∞„É°„É≥„Éà„Å†„Åë„Çí„Éï„Ç£„É´„Çø
  const segments = TEST_MODE
    ? allSegments.filter(seg => seg.start < TEST_DURATION)
    : allSegments;

  // ÊúÄÂæå„ÅÆ„Çª„Ç∞„É°„É≥„Éà„Åå5ÂàÜ„ÇíË∂Ö„Åà„ÇãÂ†¥Âêà„ÅØÂàá„ÇäË©∞„ÇÅ„Çã
  if (TEST_MODE && segments.length > 0) {
    const lastSegment = segments[segments.length - 1];
    if (lastSegment.end > TEST_DURATION) {
      lastSegment.end = TEST_DURATION;
    }
  }

  console.log(`Created ${segments.length} segments${TEST_MODE ? ' (test mode - first 5 minutes)' : ''}`);

  const beats: Beat[] = [];

  // „Éï„Çß„Éº„Ç∫1: ÂãïÁîªÂàÜÂâ≤„ÉªÊñáÂ≠óËµ∑„Åì„Åó„ÉªÁøªË®≥„ÉªË©±ËÄÖË≠òÂà•
  console.log('\nüìã Phase 1: Video Processing and Transcription');
  console.log('=========================================');
  console.log(`   Whisper Concurrency: ${CONCURRENCY.whisper} parallel requests`);
  console.log(`   Translation Concurrency: ${CONCURRENCY.translation} parallel requests`);
  console.log(`   Speaker ID Concurrency: ${CONCURRENCY.speakerId} parallel requests`);
  console.log(`   Processing all segments in parallel with individual API limits`);

  // ÂÖ®„Çª„Ç∞„É°„É≥„Éà„Çí‰∏¶ÂàóÂá¶ÁêÜÔºàÂêÑAPIÂëº„Å≥Âá∫„Åó„ÅØ„É™„Éü„ÉÉ„Çø„Éº„ÅßÂà∂ÈôêÔºâ
  const segmentPromises = segments.map(async (segment, index) => {
    const segmentNum = index + 1;
    const duration = segment.end - segment.start;
    const videoOutput = path.join(videoOutputDir, `${segmentNum}.mp4`);
    const audioOutput = path.join(videoOutputDir, `${segmentNum}.mp3`);
    const thumbnailOutput = path.join(videoOutputDir, `${segmentNum}.jpg`);
    const videoFileName = `${segmentNum}.mp4`;
    const cachedBeat = existingBeatsCache.get(videoFileName);

    // ÂãïÁîª„ÉªÈü≥Â£∞Âá¶ÁêÜÔºà„É™„Éü„ÉÉ„Çø„Éº„Å™„Åó - „É≠„Éº„Ç´„É´Âá¶ÁêÜÔºâ
    await generateVideoAndThumbnail(videoOutput, thumbnailOutput, INPUT_VIDEO, segment.start, duration);
    await extractAudioIfNeeded(audioOutput, INPUT_VIDEO, segment.start, duration);

    // Êõ∏„ÅçËµ∑„Åì„Åó„Å®ÁøªË®≥ÔºàWhisper„É™„Éü„ÉÉ„Çø„ÉºÈÅ©Áî®Ôºâ
    const multiLinguals = await API_LIMITERS.whisper(() =>
      getTranscriptionAndTranslation(cachedBeat, audioOutput, existingTranslations, DEFAULT_LANG)
    );

    // Ë©±ËÄÖË≠òÂà•ÔºàSpeakerID„É™„Éü„ÉÉ„Çø„ÉºÈÅ©Áî®Ôºâ
    const speaker = await API_LIMITERS.speakerId(() =>
      identifySpeaker(cachedBeat, multiLinguals)
    );

    return createBeatFromSegment(segmentNum, segment, multiLinguals, speaker);
  });

  const processedBeats = await Promise.all(segmentPromises);
  beats.push(...processedBeats);

  // ÈÄ≤Êçó„Çí‰øùÂ≠ò
  await saveProgress(outputPath, beats, processDuration, segments.length);

  // „Éï„Çß„Éº„Ç∫2: TTSÈü≥Â£∞ÁîüÊàê
  console.log('\n\nüé§ Phase 2: Japanese TTS Audio Generation');
  console.log('=========================================');
  console.log(`   Concurrency: ${CONCURRENCY.tts} parallel requests`);

  const ttsPromises = beats.map((beat, index) => {
    const segmentNum = index + 1;
    const jaAudioOutput = path.join(videoOutputDir, `${segmentNum}_ja.mp3`);
    return API_LIMITERS.tts(() => {
      console.log(`üîä Processing TTS for segment ${segmentNum}/${beats.length}...`);
      return generateJapaneseTTS(jaAudioOutput, beat.multiLinguals.ja);
    });
  });

  await Promise.all(ttsPromises);

  // „Éï„Çß„Éº„Ç∫3: „Çª„Ç∞„É°„É≥„ÉàÈáçË¶ÅÂ∫¶Ë©ï‰æ°
  console.log('\n\nüìä Phase 3: Segment Importance Evaluation');
  console.log('=========================================');

  // ÂÖ®„Çª„Ç∞„É°„É≥„Éà„Å´Ë©ï‰æ°„Éá„Éº„Çø„Åå„ÅÇ„Çã„Åã„ÉÅ„Çß„ÉÉ„ÇØ
  const needsEvaluation = beats.some(
    beat => beat.importance === undefined || beat.category === undefined || beat.summary === undefined
  );

  if (needsEvaluation) {
    console.log('üîç Evaluating segment importance...');

    try {
      const evaluations = await evaluateSegments(beats);

      // Ë©ï‰æ°ÁµêÊûú„ÇíÂêÑBeat„Å´ËøΩÂä†
      beats.forEach((beat, index) => {
        const segmentNum = index + 1;
        const evaluation = evaluations.get(segmentNum);
        if (evaluation) {
          beat.importance = evaluation.importance;
          beat.category = evaluation.category;
          beat.summary = evaluation.summary;
        }
      });

      console.log('‚úÖ Evaluation complete!');
      displayEvaluationStats(beats);
    } catch (error) {
      console.error('‚ö†Ô∏è  Evaluation failed:', error);
      console.log('   Continuing without evaluation data...');
    }
  } else {
    console.log('‚ôªÔ∏è  All segments already evaluated, skipping evaluation');
  }

  // ÊúÄÁµÇÁµêÊûú„ÇíJSON„Å®„Åó„Å¶‰øùÂ≠ò
  const output: Output = {
    lang: DEFAULT_LANG,
    totalDuration: processDuration,
    totalSegments: segments.length,
    beats: beats,
  };
  await fs.writeFile(outputPath, JSON.stringify(output, null, 2), 'utf-8');

  console.log(`\n‚ú® Processing complete!`);
  console.log(`üìÑ Results saved to ${outputPath}`);
  console.log(`üìÅ Video and audio files saved in ${videoOutputDir}/`);
  console.log(`\nüìà Summary:`);
  console.log(`   Total duration: ${processDuration.toFixed(2)}s`);
  console.log(`   Total segments: ${segments.length}`);
  console.log(`   Average segment length: ${(processDuration / segments.length).toFixed(2)}s`);

  if (TEST_MODE) {
    console.log(`\nüí° This was a test run. Run without --test flag to process the full video.`);
  }
}

main().catch((error) => {
  console.error('‚ùå Error:', error);
  process.exit(1);
});
