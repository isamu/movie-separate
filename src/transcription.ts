import OpenAI from 'openai';
import { createReadStream } from 'fs';
import { Beat, MultiLinguals } from './types.js';

// OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’é…å»¶åˆæœŸåŒ–
let openai: OpenAI;

export function getOpenAIClient(): OpenAI {
  if (!openai) {
    if (!process.env.OPENAI_API_KEY) {
      throw new Error(
        'Missing OPENAI_API_KEY environment variable. Please create a .env file with your OpenAI API key.'
      );
    }
    openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY,
    });
  }
  return openai;
}

export interface TranscriptionWithTimestamps {
  text: string;
  segments?: Array<{
    start: number;
    end: number;
    text: string;
  }>;
}

/**
 * Whisper APIã§éŸ³å£°ã‚’æ–‡å­—èµ·ã“ã—
 * @param audioPath éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
 * @param language éŸ³å£°ã®è¨€èª ('en' | 'ja')
 */
export async function transcribeAudio(
  audioPath: string,
  language: string = 'en'
): Promise<string> {
  const client = getOpenAIClient();
  const transcription = await client.audio.transcriptions.create({
    file: createReadStream(audioPath),
    model: 'whisper-1',
    language: language,
  });

  return transcription.text;
}

/**
 * Whisper APIã§éŸ³å£°ã‚’æ–‡å­—èµ·ã“ã—ï¼ˆverbose_jsonå½¢å¼ã§ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å–å¾—ï¼‰
 */
export async function transcribeAudioWithTimestamps(
  audioPath: string
): Promise<TranscriptionWithTimestamps> {
  const client = getOpenAIClient();
  const transcription = await client.audio.transcriptions.create({
    file: createReadStream(audioPath),
    model: 'whisper-1',
    language: 'ja',
    response_format: 'verbose_json',
    timestamp_granularities: ['segment'],
  });

  return {
    text: transcription.text,
    segments: (transcription as any).segments,
  };
}

export interface SpeakerSegment {
  speaker: string;
  text: string;
  startTime?: number;
  endTime?: number;
}

/**
 * GPT-4oã‚’ä½¿ã£ã¦è©±è€…ã‚’è­˜åˆ¥
 * ä¼šè©±ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚„å£èª¿ã‹ã‚‰è©±è€…ã‚’æ¨å®š
 */
export async function identifySpeakers(
  transcriptionText: string
): Promise<SpeakerSegment[]> {
  try {
    const client = getOpenAIClient();
    const completion = await client.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        {
          role: 'system',
          content: `You are an expert at analyzing conversation transcripts and identifying different speakers.
Parse the given Japanese conversation and separate it into individual utterances with speaker labels.
Return ONLY a valid JSON object with a "speakers" array containing objects with "speaker" and "text" fields.
Use speaker names like "è©±è€…A", "è©±è€…B", etc. for Japanese conversations.
If you cannot identify multiple speakers, return all text under one speaker.

Example format:
{
  "speakers": [
    {"speaker": "è©±è€…A", "text": "ã“ã‚“ã«ã¡ã¯"},
    {"speaker": "è©±è€…B", "text": "ã¯ã„ã€ã“ã‚“ã«ã¡ã¯"}
  ]
}`,
        },
        {
          role: 'user',
          content: `Please analyze this conversation transcript and identify the different speakers:\n\n${transcriptionText}`,
        },
      ],
      response_format: { type: 'json_object' },
    });

    const result = JSON.parse(completion.choices[0].message.content || '{"speakers":[]}');

    if (!result.speakers || result.speakers.length === 0) {
      return [{ speaker: 'è©±è€…A', text: transcriptionText }];
    }

    return result.speakers;
  } catch (error) {
    console.warn('Failed to identify speakers:', error);
    return [{ speaker: 'è©±è€…A', text: transcriptionText }];
  }
}

/**
 * ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãæ–‡å­—èµ·ã“ã—ã‹ã‚‰ä¸»è¦ãªè©±è€…ã‚’æ¨å®š
 */
export async function identifyMainSpeaker(
  transcription: TranscriptionWithTimestamps
): Promise<string> {
  const speakerSegments = await identifySpeakers(transcription.text);

  if (speakerSegments.length > 0) {
    return speakerSegments[0].speaker;
  }

  return 'è©±è€…A';
}

/**
 * ãƒ†ã‚­ã‚¹ãƒˆã‚’ç¿»è¨³
 * @param text ç¿»è¨³å…ƒãƒ†ã‚­ã‚¹ãƒˆ
 * @param fromLang ç¿»è¨³å…ƒè¨€èª
 * @param toLang ç¿»è¨³å…ˆè¨€èª
 */
export async function translateText(
  text: string,
  fromLang: string,
  toLang: string
): Promise<string> {
  try {
    const client = getOpenAIClient();
    const langNames = { en: 'English', ja: 'Japanese' };
    const completion = await client.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [
        {
          role: 'system',
          content: `You are a professional translator. Translate the given ${langNames[fromLang as keyof typeof langNames]} text to natural ${langNames[toLang as keyof typeof langNames]}. Only return the translated text, nothing else.`,
        },
        {
          role: 'user',
          content: text,
        },
      ],
    });

    return completion.choices[0].message.content || text;
  } catch (error) {
    console.warn(`Failed to translate from ${fromLang} to ${toLang}:`, error);
    return text;
  }
}

/**
 * éŸ³å£°ã‚’æ–‡å­—èµ·ã“ã—ã—ã¦æ—¥è‹±ä¸¡æ–¹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™
 * @param audioPath éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
 * @param sourceLang éŸ³å£°ã®å…ƒè¨€èª ('en' | 'ja')
 * @param translationCache ç¿»è¨³ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆå…ƒè¨€èª -> ç¿»è¨³å…ˆè¨€èªï¼‰
 */
export async function transcribeAudioBilingual(
  audioPath: string,
  sourceLang: string = 'en',
  translationCache?: Map<string, string>
): Promise<MultiLinguals> {
  const sourceText = await transcribeAudio(audioPath, sourceLang);
  const targetLang = sourceLang === 'en' ? 'ja' : 'en';

  const getCachedTranslation = () => {
    console.log(`    â™»ï¸  Using cached translation`);
    return translationCache!.get(sourceText)!;
  };

  const getNewTranslation = async () => {
    console.log(`    ğŸŒ Translating from ${sourceLang} to ${targetLang}...`);
    return await translateText(sourceText, sourceLang, targetLang);
  };

  const translatedText = translationCache?.has(sourceText)
    ? getCachedTranslation()
    : await getNewTranslation();

  return sourceLang === 'en'
    ? { en: sourceText, ja: translatedText }
    : { ja: sourceText, en: translatedText };
}

/**
 * ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éŸ³å£°ã‚’ç”Ÿæˆï¼ˆTTSï¼‰
 */
export async function textToSpeech(
  text: string,
  outputPath: string,
  language: 'ja' | 'en' = 'ja'
): Promise<void> {
  try {
    const client = getOpenAIClient();

    // è¨€èªã«å¿œã˜ã¦éŸ³å£°ã‚’é¸æŠ
    const voice = language === 'ja' ? 'alloy' : 'alloy'; // OpenAI TTSã¯å¤šè¨€èªå¯¾å¿œ

    const mp3 = await client.audio.speech.create({
      model: 'tts-1',
      voice: voice,
      input: text,
    });

    const buffer = Buffer.from(await mp3.arrayBuffer());
    const fs = await import('fs/promises');
    await fs.writeFile(outputPath, buffer);

    console.log(`    ğŸ”Š Generated ${language.toUpperCase()} audio: ${outputPath}`);
  } catch (error) {
    console.warn(`Failed to generate TTS for ${language}:`, error);
    throw error;
  }
}
